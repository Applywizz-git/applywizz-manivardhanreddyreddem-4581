

// export const personalInfo = {
//   name: "Vamshi Krishna Challa",
//   title: "Data Scientist | ML Engineer | AI Researcher",
//   email: "vamshichalla975@gmail.com",
//   phone: "+1 318-957-2204",
//   location: "Ruston, LA",
//   linkedin: "https://www.linkedin.com/in/vamshikrishna-challa-98b306292",
//   github: "https://github.com/vamshichallabb",
//   portfolio: "https://applywizz-vamshikrishnachalla.vercel.app",
//   resume: "/vamshikrishna_resume.pdf",
//     image: "/images/image.png",
// };

// export const about = {
//   summary: "I am a Data Scientist with 4+ years of experience in Data Science, Machine Learning, AI, and Big Data analytics, with expertise in industrial IoT, predictive analytics, and generative AI solutions. Skilled in Python, SQL, PySpark, TensorFlow, PyTorch, Hugging Face Transformers, AWS, and Spark, I have hands-on experience in building scalable ML pipelines, anomaly detection systems, and cloud-based deployments. My background spans time-series forecasting, NLP, computer vision, and ETL/data engineering workflows, consistently delivering actionable insights and improving predictive model performance by 15–20%. I am also experienced in MLOps practices, CI/CD pipelines, and visualization with Power BI, Tableau, and interactive dashboards to support real-time decision-making. Passionate about leveraging AI and ML for automation, research, and analytics, I actively seek opportunities in Data Science, Machine Learning Development, Generative AI, and advanced research in Computer Science..",
// };

// export const experience = [
//   {
//     title: "AI & Data Science Research Assistant",
//     company: "Grambling State University",
//     location: "Grambling, LA",
//     period: "Aug 2025 – Present",
//     description: [
//       "Conducting advanced research in AI and Data Science",
//       "Developing innovative machine learning solutions",
//       "Contributing to academic publications and research projects",
//     ],
//   },
//   {
//     title: "Data Scientist – Graduate Research Assistant",
//     company: "Louisiana Tech University",
//     location: "Ruston, LA",
//     period: "Dec 2022 – Jul 2025",
//     description: [
//       "Led research on industrial IoT anomaly detection using generative AI",
//       "Developed time-series forecasting models for smart manufacturing",
//       "Published research findings in academic conferences",
//       "Mentored undergraduate students in ML projects",
//     ],
//   },
//   {
//     title: "Machine Learning Engineer",
//     company: "Tata Consultancy Services",
//     location: "India",
//     period: "Jul 2021 – Dec 2022",
//     description: [
//       "Designed and deployed ML models for production environments",
//       "Optimized model performance and reduced latency by 30%",
//       "Collaborated with cross-functional teams on AI initiatives",
//       "Implemented MLOps best practices and CI/CD pipelines",
//     ],
//   },
//   {
//     title: "Student Intern – IoT & Smart Grids",
//     company: "Malla Reddy College of Engineering & Technology",
//     location: "India",
//     period: "Jan 2021 – May 2021",
//     description: [
//       "Developed IoT solutions for smart grid monitoring",
//       "Created dashboards for real-time fault detection",
//       "Reduced fault response time by 20%",
//     ],
//   },
// ];

// export const projects = [
//   {
//     title: "Industrial IoT Anomaly Detection with Generative AI",
//     description: "Hybrid anomaly detection framework combining LLMs with traditional ML models for industrial IoT systems.",
//     achievements: [
//       "Improved early detection accuracy by 20%",
//       "Reduced false positives by 35%",
//       "Deployed on AWS SageMaker & Lambda for real-time processing",
//     ],
//     technologies: ["Python", "PyTorch", "Hugging Face", "AWS SageMaker", "Lambda", "IoT Core"],
//     image: "project-iot",
//   },
//   {
//     title: "Time-Series Forecasting for Smart Manufacturing",
//     description: "LSTM and GRU-based models for predictive maintenance and production scheduling optimization.",
//     achievements: [
//       "Increased scheduling accuracy by 15%",
//       "Reduced computation time by 22%",
//       "Implemented feature engineering pipeline for time-series data",
//     ],
//     technologies: ["TensorFlow", "Keras", "PySpark", "Pandas", "Scikit-learn"],
//     image: "project-forecasting",
//   },
//   {
//     title: "GSM-Based Automatic Fault Detection in Overhead Lines",
//     description: "IoT-enabled fault detection system with real-time monitoring dashboards for electrical grid infrastructure.",
//     achievements: [
//       "Reduced fault response time by 20%",
//       "Improved predictive maintenance scheduling",


// "Validated prototype under simulated faults for speed and reliability.",
//     ],
//     technologies: ["IoT", "Python", "Arduino", "GSM", "Data Visualization"],
//     image: "project-fault",
//   },
// ];

// export const skills = {
//   "AI & Machine Learning": [
//     "TensorFlow",
//     "PyTorch",
//     "Scikit-learn",
//     "Hugging Face",
//     "Generative AI",
//     "LangChain",
//   ],
//   "Data Science & Analytics": [
//     "NLP",
//     "Computer Vision",
//     "Time-Series Forecasting",
//     "Predictive Analytics",
//     "Statistical Modeling",
//   ],
//   "Big Data & Cloud": [
//     "AWS (S3, SageMaker, Lambda)",
//     "Apache Spark",
//     "Databricks",
//     "PySpark",
//   ],
//   "Data Engineering": [
//     "ETL Pipelines",
//     "Airflow",
//     "Kafka",
//     "Feature Engineering",
//     "Data Lakes",
//   ],
//   "Visualization": [
//     "Tableau",
//     "Power BI",
//     "Matplotlib",
//     "Seaborn",
//     "Plotly",
//   ],
//   "MLOps & Deployment": [
//     "Docker",
//     "Kubernetes",
//     "MLflow",
//     "Weights & Biases",
//     "CI/CD",
//   ],
//   "Programming & Databases": [
//     "Python",
//     "SQL",
//     "R",
//     "Java",
//     "PostgreSQL",
//     "MongoDB",
//   ],
//   "APIs & Frameworks": [
//     "REST APIs",
//     "Flask",
//     "FastAPI",
//     "Streamlit",
//   ],
// };

// export const certifications = [
//   {
//     name: "AWS Certified Cloud Practitioner",
//     organization: "Amazon Web Services",
//     credential: "AWS-CCP",
//   },
//   {
//     name: "AWS Certified Solutions Architect – Associate",
//     organization: "Amazon Web Services",
//     credential: "AWS-SAA",
//   },
//   {
//     name: "AWS AI Practitioner",
//     organization: "Amazon Web Services",
//     credential: "AWS-AIP",
//   },
//   {
//     name: "Digital Cloud Onboard",
//     organization: "AWS",
//     credential: "Data, Analytics, ML/AI",
//   },
//   {
//     name: "Data Science Foundations: Python",
//     organization: "LinkedIn Learning",
//     credential: "LinkedIn",
//   },
//   {
//     name: "Machine Learning for All",
//     organization: "Coursera",
//     credential: "Coursera",
//   },
// ];

// export const education = [
//   {
//     degree: "Ph.D. in Computational Analysis & Modeling",
//     university: "Louisiana Tech University",
//     location: "Ruston, LA",
//     year: "Mar 2024 – Present",
//   },
//   {
//     degree: "Master of Science in Computer Science",
//     university: "Louisiana Tech University",
//     location: "Ruston, LA",
//     year: "Dec 2022 – Mar 2024",
//   },
//   {
//     degree: "Bachelor of Technology in Electrical & Electronics Engineering",
//     university: "JNTU Hyderabad",
//     location: "India",
//     year: "Jul 2017 – May 2021",
//   },
// ];

















export const personalInfo = {
  name: "MANIVARDHAN REDDY",
  title: "Data Engineer | Data Platform Engineer | Analytics Engineering",
  email: "manivardhanreddyreddem93@gmail.com",
  phone: "+1 (618) 517-5957",
  location: "Saint Louis, MO",
  linkedin: "https://www.linkedin.com/in/manivardhanreddy",
  github: "https://github.com/manivardhanreddy",
  portfolio: "https://manivardhanreddy.vercel.app",
  resume: "/manivardhan_reddy_resume.pdf",
  image: "/src/assets/image.png",
};

export const about = {
  summary:
    "Data Engineer with 4 years of experience designing, building, and operating scalable cloud data platforms, batch and streaming data pipelines, and analytics-ready data warehouses across AWS, Azure, and GCP environments. Strong expertise in SQL performance optimization, dimensional data modeling, Apache Spark, Hadoop, Kafka, Airflow, and cloud-native ETL/ELT frameworks, supporting high-volume analytical and operational workloads. Proven ability to deliver reliable, production-grade data systems through DataOps practices, CI/CD automation, and infrastructure-as-code, while enabling BI and analytics consumption through governed datasets.",
};

export const experience = [
  {
    title: "Data Platform Engineer",
    company: "Juniper Networks",
    location: "Remote",
    period: "Feb 2025 - Present",
    description: [
      "Redesigned the MySQL analytics layer by introducing table partitioning, optimized indexing strategies, and SQL query refactoring, which improved query response times by 35% while supporting 500K+ daily analytical and operational queries.",
      "Implemented scalable batch ingestion pipelines using Python, PySpark, Apache Spark, and Hadoop to process 2M+ daily network telemetry records, ensuring reliable data availability and consistent schema enforcement across analytics workloads.",
      "Established analytics-ready dimensional models using star and snowflake schemas in SQL, enabling faster BI queries and significantly reducing repeated ad-hoc transformations across Tableau, Power BI, and Looker datasets.",
      "Introduced data quality validation and reconciliation logic within transformation workflows using Python and SQL, leading to a ~30% reduction in reporting inconsistencies and improved confidence in network performance metrics.",
      "Delivered near-real-time KPI datasets by implementing Spark Structured Streaming for selected telemetry feeds, improving metric freshness by 25% for operations teams responsible for network capacity and reliability planning.",
      "Strengthened DataOps reliability by managing SQL and Python pipeline code through GitHub-backed CI/CD workflows, resulting in more predictable releases and fewer manual intervention issues during deployments.",
      "Converted network engineering and operations requirements into production-grade data platform components through Agile collaboration in Jira and Confluence, accelerating delivery of analytics features tied to network health monitoring.",
      "Improved downstream analytics efficiency by publishing governed, performance-optimized datasets to BI tools, reducing dashboard refresh latency and lowering analyst dependence on custom extracts by over 20%.",
    ],
  },
  {
    title: "Data Engineer",
    company: "Datamatics",
    location: "Remote",
    period: "Jul 2021 - Nov 2022",
    description: [
      "Built a scalable AWS data lake with Amazon S3, AWS Glue, and Amazon Redshift, centralizing 50TB+ of monthly data and enabling consistent, enterprise-wide analytics and reporting.",
      "Processing bottlenecks were eliminated by redesigning Apache Spark and PySpark batch pipelines, which cut overall transformation runtimes by 40% and improved data availability for downstream consumers.",
      "Reliable data delivery was achieved through Apache Airflow-orchestrated ETL and ELT workflows, where dependency management and retry controls sustained 99% pipeline success rates.",
      "Near-real-time analytics became possible by integrating Apache Kafka with Spark Streaming, increasing data freshness by 25% for operational dashboards and analytics use cases.",
      "Legacy warehouse workloads were modernized through migration to Amazon Redshift, where schema redesign and optimized sort and distribution keys delivered a 60% improvement in query performance.",
      "High availability across data stores was maintained by tuning PostgreSQL, MongoDB, and Redshift systems, strengthening performance stability and safeguarding data integrity.",
      "Deployment cycles accelerated by 18% after introducing GitHub-based version control and Jenkins CI/CD pipelines, reducing manual deployment errors across data engineering workflows.",
      "Analytics and data science teams received curated, analytics-ready datasets built with SQL and Spark, improving consistency and reliability for reporting, forecasting, and model development.",
    ],
  },
  {
    title: "Data Analyst - Associate (Analytics Engineering Focus)",
    company: "3i Infotech",
    location: "India",
    period: "Jul 2020 - Jun 2021",
    description: [
      "Analytics adoption grew 28% after delivering SQL Server-backed datasets engineered with optimized SQL transformations that fed Tableau and Power BI reporting for sales, marketing, and operations.",
      "Performance bottlenecks were removed by indexing and refactoring complex SQL Server queries, cutting average query latency by 32% for recurring BI workloads.",
      "Business KPIs became consistent when 500K+ transaction records were analyzed with Python, Pandas, and NumPy, translating exploratory findings into standardized reporting logic.",
      "Reporting turnaround improved through SQL-based customer segmentation and behavior models, allowing reusable datasets to power multiple dashboards without repeated transformations.",
      "Manual effort declined after automating data preparation and transformations with SQL and Python, resulting in cleaner handoffs to downstream analytics consumers.",
      "Stakeholder questions were converted into scalable analytics datasets by aligning requirements with data platform standards, improving long-term maintainability of reporting logic.",
      "Onboarding time shortened once data models, SQL logic, and workflows were documented in Confluence, enabling faster ramp-up for analysts and junior engineers.",
      "Confidence in reports increased by embedding validation and reconciliation checks within SQL pipelines, supporting audit-ready outputs and reliable enterprise analytics.",
    ],
  },
  {
    title: "Data Analyst Intern",
    company: "3i Infotech",
    location: "India",
    period: "Jul 2018 - Dec 2018",
    description: [
      "Manual processing effort dropped by 10+ hours per month after scripting recurring data tasks with Python and Java, allowing analytics teams to deliver reports faster and with fewer delays.",
      "Data accuracy increased by 20% when Oracle tables were validated and maintained using SQL checks, preventing inconsistent records from reaching reporting and analytics layers.",
      "Hands-on experience with large-scale data processing was gained by supporting Scala and Apache Spark transformations on Azure Databricks, enabling reliable batch ETL for enterprise datasets.",
      "Trend identification accelerated through exploratory analysis built with Python and Plotly, helping analysts surface anomalies and patterns earlier in the analysis lifecycle.",
      "Analytics workloads became more scalable by assisting with Azure-based data platform deployments, supporting storage and compute configurations used for reporting and transformation pipelines.",
      "Repeated data issues were reduced by collaborating with senior engineers to resolve SQL and Spark pipeline quality and performance problems, improving overall pipeline stability.",
    ],
  },
];

export const projects = [
  {
    title: "Network Performance Analytics & Database Optimization Platform",
    description:
      "Built scalable analytics pipelines using Apache Spark, SQL, and Python to process high-volume network latency, throughput, and traffic logs, enabling reliable performance monitoring across large telemetry datasets.",
    achievements: [
      "Query performance improved by 40% after redesigning MySQL schemas, indexing strategies, and execution paths with analytical SQL.",
      "Operational visibility increased by publishing optimized datasets to Tableau and Power BI, enabling faster incident detection and root-cause analysis.",
      "Implemented dimensional modeling with star and snowflake schemas for faster BI queries.",
    ],
    technologies: ["Apache Spark", "MySQL", "Python", "SQL", "Tableau", "Power BI", "PySpark"],
    image: "project-network",
  },
  {
    title: "Cloud Data Platform Migration & Real-Time Streaming Architecture",
    description:
      "Migrated enterprise ETL workflows to an AWS-native data platform using Amazon S3, AWS Glue, EMR, Lambda, and Redshift, improving scalability and standardizing cloud-based data processing.",
    achievements: [
      "Enabled sub-minute analytics by implementing event-driven pipelines with AWS Kinesis and Spark Streaming.",
      "Deployment consistency improved through Terraform-based infrastructure provisioning and CI/CD pipelines.",
      "Centralized 50TB+ of monthly data enabling consistent, enterprise-wide analytics and reporting.",
    ],
    technologies: ["AWS S3", "AWS Glue", "Redshift", "Kinesis", "Spark Streaming", "Terraform", "Lambda"],
    image: "project-cloud",
  },
  {
    title: "Enterprise Customer Analytics & BI Enablement",
    description:
      "Unified CRM, marketing, and transaction data by designing analytics-ready dimensional models with SQL, creating a centralized warehouse optimized for analytical queries.",
    achievements: [
      "Customer insights became more actionable after delivering curated datasets to Tableau, Power BI, and Looker.",
      "Analytics turnaround time decreased after automating KPI refresh workflows using SQL and scheduled pipelines.",
      "Improved analytics adoption by 28% through optimized SQL transformations.",
    ],
    technologies: ["SQL Server", "Tableau", "Power BI", "Python", "Pandas", "NumPy"],
    image: "project-analytics",
  },
  {
    title: "Data Automation & Enterprise Reporting Framework",
    description:
      "Reduced manual reporting effort by developing modular ETL and ELT pipelines with Python, SQL, and Apache Airflow, ensuring reliable and repeatable data transformations.",
    achievements: [
      "Reporting consistency improved through standardized data models and reusable analytical layers.",
      "Operational visibility expanded by implementing scheduled data pipelines that delivered daily, weekly, and monthly insights.",
      "Achieved 99% pipeline success rates through dependency management and retry controls.",
    ],
    technologies: ["Apache Airflow", "Python", "SQL", "ETL", "ELT"],
    image: "project-automation",
  },
];

export const skills = {
  "Programming & Querying": ["Python", "SQL", "PySpark", "Scala", "Bash"],
  "Data Engineering & Streaming": [
    "Apache Spark (SQL, Structured Streaming)",
    "Hadoop",
    "Hive",
    "Apache Kafka",
    "AWS Kinesis",
  ],
  "Databases & Warehousing": [
    "MySQL",
    "PostgreSQL",
    "SQL Server",
    "Oracle",
    "MongoDB",
    "Amazon Redshift",
    "Snowflake",
    "Google BigQuery",
  ],
  "ETL / ELT & Orchestration": [
    "Apache Airflow",
    "AWS Glue",
    "Databricks",
    "dbt",
  ],
  "Cloud Data Platforms": [
    "AWS (S3, Redshift, Glue, EMR, Lambda, EC2, Kinesis)",
    "GCP (BigQuery, Cloud Storage, Dataflow)",
    "Azure (Databricks, Data Lake, Synapse)",
  ],
  "Data Modeling & Analytics": [
    "Dimensional Modeling",
    "Star & Snowflake Schemas",
    "Analytical SQL",
    "Data Quality Frameworks",
  ],
  "BI & Consumption": ["Tableau", "Power BI", "Looker"],
  "DataOps & Automation": [
    "Docker",
    "CI/CD",
    "GitHub",
    "Jenkins",
    "Terraform",
    "Monitoring & Alerting",
  ],
  "Delivery & Collaboration": ["Agile", "Scrum", "Jira", "Confluence"],
};

export const certifications = [
  {
    name: "Google Professional Data Engineer",
    organization: "Coursera",
  },
  {
    name: "IBM Data Engineering Professional Certificate",
    organization: "Coursera",
  },
  {
    name: "DeepLearning.AI Data Engineering Professional Certificate",
    organization: "Coursera",
  },
  {
    name: "Data Engineering Foundations",
    organization: "LinkedIn Learning",
  },
  {
    name: "Modern Data Engineering with Snowflake",
    organization: "LinkedIn Learning",
  },
  {
    name: "AI Fundamentals",
    organization: "DataCamp",
  },
  {
    name: "Certified ScrumMaster (CSM)",
    organization: "Scrum Alliance",
  },
];

export const education = [
  {
    degree: "Master of Science in Information Systems",
    university: "Belhaven University",
    location: "United States",
    year: "2022 - 2024",
  },
  {
    degree: "Bachelor of Science in Computer Science",
    university: "Chaudhary Charan Singh University",
    location: "India",
    year: "2014 - 2018",
  },
];
